[global_tags]
  domain = "AIops Autonomous System"

[agent]
  interval = "5s"
  round_interval = true
  metric_batch_size = 1000
  metric_buffer_limit = 10000
  collection_jitter = "0s"
  flush_interval = "10s"
  flush_jitter = "0s"
  precision = ""
  hostname = "telegraf-container"
  omit_hostname = false

# --- OUTPUT: INFLUXDB ---
[[outputs.influxdb_v2]]
  urls = ["http://influxdb:8086"]
  token = "my-super-secret-admin-token"
  organization = "AIops_org"
  bucket = "AIops_bucket"

# --- INPUT 1: METRICHE STANDARD ---
# Gestisce i dati numerici (CPU, Memory, Latency)
[[inputs.mqtt_consumer]]
  servers = ["tcp://mosquitto:1883"]
  topics = [
    "AIops/metrics/+/+/+"
  ]
  data_format = "json"

  # Parsing specifico per la struttura: AIops/metrics/{cluster}/{container}/{metric_name}
  [[inputs.mqtt_consumer.topic_parsing]]
    topic = "AIops/metrics/+/+/+"
    tags = "_/_/cluster/container/metric"


# --- INPUT 2: ANALYZER LLM RESPONSE ---
# Gestisce le risposte testuali dell'intelligenza artificiale
[[inputs.mqtt_consumer]]
  servers = ["tcp://mosquitto:1883"]
  
  # Topic specifico dell'Analyzer
  topics = [
    "AIops/analyzer_llm_response"
  ]
  
  # I dati arrivano come JSON {"timestamp": ..., "response": "..."}
  data_format = "json"
  
  # Salviamo questi dati sotto una measurement specifica per distinguerli dalle metriche
  name_override = "llm_analysis"

  # Diciamo a Telegraf che il campo "response" Ã¨ una stringa (InfluxDB di default preferisce numeri)
  json_string_fields = ["response"]